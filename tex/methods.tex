\section{Expert Panel}


The \emph{initial \gls{cci}} comprised the 32 items developed using the processes described earlier. We gave these items to an expert panel consisting of 11 professors with backgrounds in cybersecurity and one cybersecurity professional for review. The experts each received the initial \gls{cci} in the form of an online exam containing each of the items. We asked experts to rate each item on the scale Accept, Accept with Minor Revisions, Accept with Major Revisions, and Reject, and to comment on the item. After they answered the question, experts were shown the correct answer and given the option to provide additional comments on the correct answer. 

\iflong

The experts' comments mainly concerned the clarity of items. We addressed their comments and then checked back with them for approval of the updated item. An example of how we addressed these comments is in Section 4.1. 

For some items, the experts disagreed with the content or the correct answer. When the experts disagreed, we omitted that item from the \gls{cci}. After we removed these items, the experts' reviews were used to rate the remaining items. The highest rated items were incorporated into the current \gls{cci}.

During the expert reviews, we continued developing items for the \gls{cci}. These items were intended to be used as alternates. One alternate item, Q25, was not reviewed by experts but was included. We thought this item evaluated the core concept \textit{\gls{c}} better than those that were reviewed. This item and those with the best reviews form the current 25 item \gls{cci}.

\fi


\section{Current \gls{cci}}

We selected items with a range of difficulties based on our best estimation. The breakdown of questions is six easy, 16 medium, and three hard. The actual performance of students would likely differ from our estimations. Each item covers one of the five major concepts shown in Table \ref{tab:topics}. \iflong The items are shown in Table \ref{tab:final_question_breakdown} in Appendix 1. This table shows the scenario, name, concept, topic of each item in the current \gls{cci}.\fi  



\glsreset{c}
\glsreset{v}
\glsreset{d}
\glsreset{g}
\glsreset{t}


\begin{table}[!ht]
\centering
\caption{Five Core Concepts of Cybersecurity}
\scalebox{.65}{
\begin{tabular}{c}
\toprule
  \large{\gls{v}}\\
  \large{\gls{c}}\\
  \large{\gls{d}}\\
  \large{\gls{g}}\\
  \large{\gls{t}}\\
\bottomrule
\end{tabular}
}
\label{tab:topics}
\end{table}




\FloatBarrier






\section{Pilot Trial}

The goal of the pilot test was to administer the current \gls{cci} to a small group of 100-200 students and then use the results of this pilot test to suggest modifications to the instrument. We concluded the pilot test in December 2018 after 142 students from six universities completed the \gls{cci}.

Professors at each university had the option of administering a paper version or online version of the \gls{cci}. Both versions included the instructions seen in Appendix A. The distractors, scenarios, and questions were identical in both versions. 

The professor proctored the paper version of the exam by allocating 50 minutes for students to take the 25 item exam in class. Students then completed the exam to the best of their abilities. The professor collected the exam papers and sent them to us where we recorded each student's response to every item.  

If the professor decided to administer the online version, students were provided a link to the exam. The online version differed from the paper version in three ways. First, the online version had a random ordering of distractors. Second, items that shared a scenario were randomly ordered within that scenario. For example, if Q1 and Q2 are the two items in the one scenario, Q1 could appear before or after Q2 but always together. The reason for randomizing the online version was to dissuade collusion between students and to minimize any possible effect of item order on student performance. Because students who had access were all in the same course they may have attempted to work together even if they received no benefit from receiving a better score. Third, there was no hard time limit. Students were told to spend 50 minutes but this was not strictly enforced. The student completed the online version and then selected a submit button to save and submit their exam.


\FloatBarrier
\section{Pilot Demographics}

The universities included in the pilot trial have diverse locations and populations. Universities A and D are large Midwestern public universities and have over 40 thousand students enrolled. University E is a large public university from the Southwest with over 40 thousand students enrolled. Universities B, C, F are smaller universities from the Midwestern and Eastern part of the country. These Universities have 10 thousand or fewer students enrolled.

\ifshort
The demographics of the study including institution and response rate are in Table \ref{tab:student_breakdown}. All of the universities administered the online exam except for University A. 
\fi

\iflong
The demographics of the study including institution and response rate are in Table \ref{tab:student_breakdown}. University A was the only group given the \gls{cci} in paper format. At University D, a link to the instrument was sent to six members of a professional engineering club who were taking the course. At the other universities, a professor sent a link to the instrument to the students in the course. 
\fi


\iflong
\begin{table}[!htbp]
\caption{Breakdown of Students by University}
\centering
\scalebox{.6}{
\begin{tabular}{cS[table-number-alignment = center]S[table-number-alignment = center]S[table-number-alignment = center]}
    \toprule
    \textbf{University} & \textbf{Number of Students} & \textbf{Potential Number of Students} & \textbf{Response Rate (\%)}\\
    \midrule
    \textit{University A} & 91 & 120 & 76 \\
    \textit{University B} & 12 & 20 & 60 \\
    \textit{University C} & 1 & 12 & 16\\
    \textit{University D} & 6 & 6 & 100 \\   
    \textit{University E} & 17 & 50 & 34 \\
    %\textit{University F} & 6 & 40 & 15 \\
    \textit{University F} &    12 & 20 & 60\\
    \textit{No University Specified} & 3 &  & \\
    \midrule
    \textit{Total} & 142 & 228 & 62 \\
    \bottomrule
\end{tabular}   
}

\label{tab:student_breakdown}
\end{table}
\fi

\ifshort
\begin{table}[!htbp]
\centering
\caption{Breakdown of Students By University}
\scalebox{.8}{
\begin{tabular}{cc}
    \toprule
    \textbf{University} & \textbf{Number of Students}\\
    \textit{University A} & 91  \\
    \textit{University B} & 14  \\
    \textit{University C} & 1 \\
    \textit{University D} & 6  \\   
    \textit{University E} & 17 \\
    %\textit{University F} & 6 & 40 & 15 \\
    \textit{University F} &    12\\
    \textit{No University Specified} & 3 \\
    \textbf{\textit{Total}} & 142\\
    \bottomrule
\end{tabular}   
}
\label{tab:student_breakdown}
\end{table}
\fi


\iflong
\section{Incomplete Exams}

There were two types of null responses that we saw and discarded from students. The first was students that made no effort to complete the \gls{cci} leaving all items blank. The second was three students who began the exam but did not continue and terminated it early (completing less than 5 items). Both types of null responses were discarded. If a student completed the majority of the items but left a minority blank their results were included.

\fi
