\iflong
This thesis presents results from a validation study of the \glsdesc{cci}: a concept inventory that is intended to evaluate how well a first course in cybersecurity helps students learn core cybersecurity concepts. 
\fi
As computers become ubiquitous, controlling our money, medical equipment, cars, and personal data, it is becoming increasingly essential that engineers, computer scientists, business professionals, and policy makers have an understanding of core cybersecurity concepts. Unfortunately, there is little research on how to teach cybersecurity effectively and no validated research instruments to support future research. The validity of an educational research instrument is the set of evidence and arguments that establish whether the instrument can be appropriately used for the purposes that we claim. To establish the validity of our instrument, we are following the design and evaluation framework recommended by the National Research Council. In prior work, we focused on establishing the cognitive basis for the assessment. We selected topics for the \gls{cci} through a Delphi process and used cognitive interviews to identify and understand common misconceptions students have about cybersecurity. Based on these prior studies, we developed a draft \gls{cci} comprised of 25 questions that cover 5 concepts related to adversarial thinking: understanding the adversary; defining security goals; identifying targets, vulnerabilities, threats, and risks; and devising defenses. In this thesis, we continue the validation by evaluating this draft \gls{cci} using both expert review and psychometric analysis of student performance on the assessment.

\iflong
The \gls{cci} was reviewed by an expert panel of professors for clarity and whether each item on the \gls{cci} assesses one of our identified core concepts. We discuss this expert feedback and its implications for the future development of the \gls{cci}. We also administered this draft \gls{cci} to over 150 students from a diverse set of universities including the University of Illinois at Urbana-Champaign (UIUC), Purdue University, and Texas A\&M among others. According to Jorion et al. (2015), a valid concept inventory should have questions with a range of difficulty and each question should provide useful information about how a student understands the topic. We use \gls{ctt} to evaluate the quality of each item individually and the \gls{cci} as a whole. We report the difficulty and discrimination to evaluate the information captured by each item as well as the overall distribution of item difficulty. We also report on which distractors students chose and what these choices reveal about the assessment and studentsâ€™ understanding. We use the findings of this study to identify the successful aspects of the instruments and provide recommendations for refinements that will be made in further iterations. 
\fi