%Many \glspl{cilabel} have been developed for subjects in \gls{stem} fields \cite{ci_progress}. A \gls{cilabel} can be powerful if the instrument is valid and measure students' conceptual knowledge. However, a minority of \glspl{cilabel} have been scrutinized using measurement or test development theories to justify being considered valid and reliable \cite{dlci}.


%The \gls{cci} was analyzed using expert review and a pilot trial with 142 students. 

This validation study has revealed mixed results of the quality of the \gls{cci} at this time. The \gls{cci} has two good properties: high reliability and strong expert consensus on the suitability of all items. Unfortunately, our findings revealed a few weaknesses of the \gls{cci} as currently constructed: low cohesion for individual concepts, items that are too difficult, and too many difficulty items on the assessment.

From the results of the pilot trial, the \gls{cci} had very high reliability, especially when compared to other \glspl{cilabel}. The Cronbach's $\alpha$ is 0.78, which is considered good for a \gls{cilabel}. In addition to the assessment's reliability, no items decrease the overall $\alpha$ indicating that the individual items are all measuring the same construct of cybersecurity conceptual knowledge \cite{dlci}. The reliability of the assessment is necessary, but sufficient, for the assessment to be valid.

Each item was positively reviewed by experts. Experts also provided suggestions for improving the wording or distractors of each item. We used this feedback to select the 25 items that had the strongest consensus of quality from the experts. The expert reviews provide evidence for the content validity of the \gls{cci} by demonstrating that multiple cybersecurity instructors believe that the \gls{cci} items represent conceptual knowledge that students should have after a first course in cybersecurity. The content validity provides further evidence for the overall validity of the assessment.

\glsreset{c}

The strengths of the \gls{cci} indicate that the collection of items and individual items are well designed from an instructor perspective and reliable from a student performance perspective. However, the student response data reveals that there is still room for improvement. Notably, while the \gls{cci} was designed to assess five concepts, the student performance data did not align well with these five concepts. For example, there is no consistent correlation of the items within each concept. Additionally, the items that evaluate the concepts have low reliability; each $\alpha$ for the individual concept is below 0.5 \cite{jorian}. Because of the low reliability of the concepts, we cannot recommend using the concept subtests to assess students' knowledge of each concept individually.

There are two possible interpretations for this lack of cohesion and reliability within the concept subtests. First, it is possible that the items were poorly designed and do not reflect the core concepts. Second, it is possible that the concepts themselves are poorly bounded, interconnected, or too complex. Given that the expert reviewers did not express any concerns about the content of the items, we argue that the second interpretation is more likely. 

Our finding of low cohesion among concept subtests is a common finding among previously published \glspl{cilabel} \cite{jorian}. The commonality of this finding suggests that it is generally difficult for designers of an instrument to design effective concept subtests. While most items may primarily engage students in one concept, the concepts are likely to be interconnected. Students need to use multiple concepts to answer each item correctly. We believe that this may be especially true in cybersecurity, which requires individuals to consider the motivations or capabilities of attackers, constraints or goals of defenders, and the technologies or techniques that are needed to mitigate risk.

Additionally, the concepts discovered in the Delphi process may be too complex and are really culminations of similar, but separate, concepts \cite{delphi}. For example, concept \gls{c} involves four unique forms of attack. A confidentiality attack could cover attacking a secure message protocol, whereas an availability attack could cover a denial of service attack. Both of these examples are forms of attack and both of them are very relevant to cybersecurity. However, a student may understand mechanisms that enable secure communications and still have very little idea about denial-of-service attacks. Thus each item of the \gls{cci} must be multifaceted, meaning that creating subtests will be difficult, if not intractable, without creating isomorphic, redundant questions.

%In future iterations, it may be useful to split these concepts up into smaller concepts or use heuristics. \gls{efa} can find underlying relations between exam constructs. These heuristics will be useful with more students to explore potential concept groupings.

If we want to create reliable and valid concept subtests, we may need to consider other models for creating them. For example, we could try narrowing the scope of concept \gls{c} to just one attribute (e.g., confidentiality). This option may not be desirable because it ignores the complexity of an attacker's varied motivations. Alternatively, we could create multiple assessments that more fully explore each of the five core concepts, but this option would dramatically increase the work and cost of creating assessment tools for cybersecurity. As currently constructed, the \gls{cci} provides a reliable instrument for measuring a students' overall understanding of cybersecurity, which is a much-needed first step. Future work can explore which types of future development are needed for creating these subtests.

% If we want to measure CIA triad it may require it's own CI with more items.

Unlike the alignment of the concepts, a good range of difficulty is often achieved in published \glspl{cilabel} and necessary for the assessment to be valid. The \gls{cci} is skewed to be too difficult: 5 items are more difficult than the recommended difficulty and for 21 out of 25 items, less than 50\% of students answered each item correctly. This degree of difficulty suggests that some items need to be made easier to improve our ability to distinguish between students with varying abilities and knowledge. Future work on the \gls{cci} must explore how to effectively make some items easier to improve the quality of the \gls{cci}. 

%The items that fall below the minimum difficulty will be altered before administering the \gls{cci} to more students and are covered in future work.

\section{Limitations}

There are a number of limitations in the pilot trial. The most notable limitation is the depth of analysis performed on the pilot trial results. \gls{irt} is not practical with the number of students in the trial. Additionally, measurements like \gls{cfa} and \gls{efa} were not performed because the Cronbach's $\alpha$ for each concept was so poor and the assessment is not completed. These limitations are acceptable because this trial is the first step and will be expanded on in future iterations.

There were also limitations in the number of students from each university. Ideally, the representation from the different universities would be even so that the results were not skewed toward University A. The localization may have biased the findings to one university.

\section{Future Work}

We will take Q15 as a specific example of the type of modification we will make to the difficult items. Less than 10\% of students answered Q15 correctly, far below what is acceptable for a \gls{cilabel}. \glsreset{v}

The item covers finding vulnerabilities in a defense and falls under concept \gls{v}. The scenario describes a hypothetical nuclear treaty between two countries that requires a method of securely transmitting a message from a monitoring device. Neither country trusts the other, and the design must be fair to each country. There are certain properties the solution must hold. Both parties want assurances that the message is not modified. Country A wants to ensure that the message originates from the device. Country B wants to monitor the message data in real time. The premise is: ``The sender applies a keyed cryptographic hash function to each message using a key distributed only to the sender, Country A, and Country B." Students are expected to find potential vulnerabilities in the suggested outputs of the device.

Option A is the message with a hash of the message and the current time. Options B, C, and D are the key and a hash of the message, the message and hash of the message, and the hash of the message respectively. Option E, the correct answer, is that the design cannot satisfy the system requirements. 

Our distractor analysis revealed that the best students chose distractor A in much the same way that they choose the correct answer for other problems. This finding reveals that, as students' knowledge increased, this wrong answer became more compelling. When constructed well, each item should lead students to pick the correct answer more often as their knowledge increases.

The preference for option A is understandable given that it is more reasonable than the other three distractors. Options B and D do not even send the original message, so the message cannot be verified. Options A and C do not guarantee the source, and since each party has the key they can modify the message and attach a new hash. Because A has the same structure as C with the addition of time being sent, it appears to be strictly superior to C, making it the best option among the distractors. Students must see the problems with each distractor and select option E, which serves as a ``none of the above". Including a ``none of the above" in general makes assessments harder \cite{none_of_above}, especially with optiona A and C satisfying some of the desired properties. 

The problem with the item and ``none of the above" in general is that option makes no assertion. This leads students to pick the most reasonable of the other choices. We have modified this item, changing option E to make an assertion. The new option E is: ``The design does not work because Countries A and B can modify the message." This allows students a definitive assertion to test and come to the same conclusion that the other options do not satisfy the requirements. We anticipate that this change, while being minor, will make the item easier and thus better differentiate among students.

After making similar modifications to other items, our next work is to administer the assessment to more students and reanalyze the results. With the easier items, the difficulty will cover a better range and better separate students. The range of difficulty and modification of items that are too difficult should increase the discriminatory power of the \gls{cci}, improving its validity and usefulness. %With more data, revising the topics to be more refined and cover specific concepts. A heuristic can be used to determine how the items fit better into smaller factors and determine the actual concepts reflected.